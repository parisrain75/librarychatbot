import os
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

# LangChain ChromaDBì—ì„œ ë°œìƒí•˜ëŠ” sqlite3 ë²„ì „ ë¬¸ì œ í•´ê²°
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma


# Gemini API í‚¤ ì„¤ì •
try:
    # ì‚¬ìš©ì í™˜ê²½ì— ë”°ë¼ st.secrets ëŒ€ì‹  os.environ.get("GOOGLE_API_KEY")ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception as e:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    # ì£¼ì˜: ì´ íŒŒì¼ì€ ì‚¬ìš©ì í™˜ê²½ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆì–´ìš”. (ê·œì •ì§‘ ë¶„ì„ ì™„ë£Œ!)")

    persist_directory = "./chroma_db"
    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)")
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )
    st.success("ğŸ’¾ í•™êµ ê·œì • ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•´ë„ ë¼ìš”! ğŸ¥³")
    return vectorstore

# ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        return create_vector_store(_docs)
    
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model):
    # ğŸŒŸğŸŒŸğŸŒŸ ì´ ë¶€ë¶„ì„ ì‹¤ì œ í•™êµ ê·œì •ì§‘ PDF íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•´ ì£¼ì„¸ìš”! ğŸŒŸğŸŒŸğŸŒŸ
    # ì˜ˆì‹œ: file_path = "ìš°ë¦¬í•™êµ_ìƒí™œê·œì •ì§‘.pdf"
    file_path = "my_highschool_handbook.pdf" 
    
    # âš ï¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.
    try:
        pages = load_and_split_pdf(file_path)
    except FileNotFoundError:
        # ì¹œì ˆí•œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶œë ¥
        st.error(f"âŒ ì˜¤ë¥˜: ì§€ì •ëœ ê²½ë¡œì— íŒŒì¼ '{file_path}'ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì¶”ê°€í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ PDF íŒŒì¼ì„ ì•±ì´ ì‹¤í–‰ë˜ëŠ” í™˜ê²½ì— ë„£ì€ í›„, 89ë²ˆì§¸ ì¤„ì˜ íŒŒì¼ëª…ì„ ì •í™•íˆ ì¼ì¹˜ì‹œì¼œì£¼ì„¸ìš”!")
        st.stop()
        
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬, \
    ëŒ€í™” ê¸°ë¡ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ì„ ìƒˆë¡­ê²Œ êµ¬ì„±í•´ì£¼ì„¸ìš”. \
    ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì§€ ë§ˆì„¸ìš”. í•„ìš”í•œ ê²½ìš° ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì„±í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - **ê³ ë“±í•™ìƒ ì±—ë´‡ ì»¨ì…‰ ìœ ì§€**
    qa_system_prompt = """ë‹¹ì‹ ì€ ë°œë„í•˜ê³  ì¹œì ˆí•œ ê³ ë“±í•™êµ ì„ ë°° ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤. \
    ì‚¬ìš©ìê°€ ì œê³µí•˜ëŠ” í•™êµ ê·œì •(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ì‹ ì†í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. \
    ë‹µë³€ì€ í•­ìƒ 'ì¹œê·¼í•œ ìš”ì²´'ë¥¼ ì‚¬ìš©í•˜ë©°, ë‚´ìš©ì´ ì™„ë²½í•˜ê³  ì •í™•í•˜ë„ë¡ ë…¸ë ¥í•´ì£¼ì„¸ìš”. \
    ë‹µë³€ì—ëŠ” ì ì ˆí•˜ê³  ê·€ì—¬ìš´ ì´ëª¨í‹°ì½˜ (ğŸ’–, ğŸ¥³, âœ¨ ë“±)ì„ ê¼­ í¬í•¨ì‹œì¼œ í™œë ¥ì„ ë”í•´ì£¼ì„¸ìš”! \
    ë§Œì•½ ì£¼ì–´ì§„ contextì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, 'ìŒ... ì œê°€ ê°€ì§„ ì •ë³´ë¡œëŠ” í™•ì‹¤íˆ ì•Œ ìˆ˜ ì—†ëŠ” ë‚´ìš©ì¸ê±¸ìš” ğŸ§'ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.\

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    try:
        llm = ChatGoogleGenerativeAI(
            model=selected_model,
            temperature=0.7,
            convert_system_message_to_human=True # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ LLMì—ê²Œ ë” ì˜ ì „ë‹¬
        )
    except Exception as e:
        st.error(f"âŒ Gemini ëª¨ë¸ '{selected_model}' ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.info("ğŸ’¡ 'gemini-2.5-flash' ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
        raise
        
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI - **ê³ ë“±í•™ìƒ ì±—ë´‡ ì»¨ì…‰ ìœ ì§€**
st.header("âœ¨ êµë‚´ ìƒí™œ ë§Œë ™ ì°ê¸°! ë˜‘ë˜‘í•œ ìŠ¤ì¿¨ í”Œë˜ë„ˆ ë´‡ ğŸ¤–")
st.markdown("_{tip: PDF íŒŒì¼ì„ ì‹¤ì œ í•™êµ ê·œì •ì§‘ìœ¼ë¡œ êµì²´í•˜ë©´ ë” ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”!}_")

# ì²« ì‹¤í–‰ ì•ˆë‚´ ë©”ì‹œì§€
if not os.path.exists("./chroma_db"):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ì…ë‹ˆë‹¤. ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê·œì •ì§‘ ë¶„ì„ ì¤‘... (ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!)")
    st.info("ğŸ’¡ ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” í›¨ì”¬ ë¹ ë¥´ê²Œ ì±—ë´‡ì„ ë§Œë‚  ìˆ˜ ìˆì–´ìš”! ğŸ¥³")

# Gemini ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select Gemini Model",
    ("gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-exp"),
    index=0,
    help="Gemini 2.5 Flashê°€ ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤"
)

try:
    with st.spinner("ğŸ”§ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë´ìš”! ğŸ’–")
except Exception as e:
    # initialize_componentsì—ì„œ ì´ë¯¸ ì—ëŸ¬ë¥¼ ì¶œë ¥í–ˆì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ í•œ ë²ˆ ë” ì¶œë ¥
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.info("PDF íŒŒì¼ ê²½ë¡œì™€ GOOGLE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ëŒ€í™” ê¸°ë¡ì´ í†µí•©ëœ RAG ì²´ì¸ êµ¬ì¶•
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)


if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", 
                                     # **ì´ˆê¸° ì¸ì‚¬ë§ ìˆ˜ì •**
                                     "content": "ì•ˆë…•! âœ¨ í•™êµìƒí™œ ë§ˆìŠ¤í„° ë´‡ì´ì•¼! ê¶ê¸ˆí•œ ê·œì •ì´ë‚˜ í•™ì‚¬ ê¿€íŒì´ ìˆë‹¤ë©´ ë­ë“ ì§€ ë¬¼ì–´ë´! ğŸ’–"}]

# ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)


if prompt_message := st.chat_input("ê·œì •ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”."):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("ìƒê° ì¤‘... ì ì‹œë§Œìš”! ğŸ¤”"):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config)
            
            answer = response['answer']
            st.write(answer)
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸ ğŸ‘€"):
                for doc in response['context']:
                    st.markdown(f"**ì¶œì²˜:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} (í˜ì´ì§€: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')})", help=doc.page_content)
